\documentclass[usenames,dvipsnames,notheorems]{beamer}

\usefonttheme[onlymath]{serif}

% silence annoying warnings
\usepackage{silence}
\usepackage{caption}
\WarningFilter{remreset}{The remreset package}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{centernot}
\usepackage{dsfont}

\input{macros/math}
\input{macros/plots}

\usepackage{simplebeam}
\usetheme{simplebeamer}

\usetikzlibrary{shapes, arrows}
\usetikzlibrary{decorations.pathreplacing, calligraphy}

% node styles
\tikzstyle{Input}=[minimum size=0.5cm, fill=none, line width = 0.5mm, draw=black, shape=circle, text=black]
\tikzstyle{Hidden}=[minimum size=0.4cm, fill=blue, line width = 0.5mm, draw=blue, shape=circle, text=black]
\tikzstyle{Splits}=[inner sep=0.03cm, minimum size=0.3cm, line width = 0.3mm, draw=blue, shape=circle, text=black]
\tikzstyle{Output}=[minimum size=0.3cm, fill=white, line width = 0.5mm, draw=black, shape=circle, text=black]

% Edge styles
\tikzstyle{arrow}=[line width = 0.5mm]

% bib resources

\addbibresource[]{refs.bib}

\title{Convex Analysis of Non-Convex Neural Networks}
\author{Aaron Mishkin\\ \vspace{1.5ex} {\small Supervised by Mert Pilanci}}
\collaborators{
	\includegraphics[width=0.3\linewidth]{assets/aaron_excited.jpeg}
	\includegraphics[width=0.3\linewidth]{assets/mert.jpg}
}

\titlegraphic{\includegraphics[width=0.4\textwidth]{assets/SUSig_2color_Stree_Left.eps}}

\newcommand{\horizontalrule}{
	{
			\vspace{-0.5em}
			\center \rule{\textwidth}{0.1em}
			\vspace{-0.2em}
		}
}

\newcommand{\red}[1]{\textcolor{Red}{#1}}
\newcommand{\green}[1]{\textcolor{ForestGreen}{#1}}
\newcommand{\blue}[1]{\textcolor{DarkBlue}{#1}}
\newcommand{\purple}[1]{\textcolor{Magenta}{#1}}

\definecolor{bad}{HTML}{eb6223}
\definecolor{good}{HTML}{9434ed}

\newcommand{\bad}[1]{\textcolor{bad}{#1}}
\newcommand{\good}[1]{\textcolor{good}{#1}}

\definecolor{sanddune}{rgb}{0.59, 0.44, 0.09}
\definecolor{byzantine}{rgb}{0.59, 0.44, 0.09}
\definecolor{ultramarine}{rgb}{0.07, 0.04, 0.56}
\definecolor{brightpink}{rgb}{1.0, 0.0, 0.5}

% toggle plotting tikz
\def\showtikz{}

%\logo{\includegraphics[height=0.5cm]{assets/Block_S_2_color.eps}}

%\institute{Stanford University}
\frenchspacing
\date{}

\begin{document}

\maketitle
%% main content starts %%

\setbeamercolor{background canvas}{bg=LightCyan}

\begin{frame}{}
    \begin{center}
        \huge 1. Motivation: Convexity and Deep Learning
    \end{center}
\end{frame}
\setbeamercolor{background canvas}{bg=white}

\begin{frame}{Motivation: Thirteen Years Since AlexNet}

    {
        \large \textbf{13 Years Ago}: AlexNet won ILSVRC 2012 and started
        the modern ``deep learning'' era of machine learning.
    }
    \pause

    \vspace{0.5em}

    \horizontalrule

    \vspace{0.5em}

    AlexNet improved over the next best model by \( \approx 10\% \) (top-5).

    \vspace{1em}
    \pause

    \textbf{Key Techniques}:
    \pause
    \begin{itemize}
        \item ``a \good{large}, deep convolutional neural network''
              \pause
        \item ``a very \good{efficient} GPU implementation of convolutional
              nets''
              \pause
        \item ``'dropout', a recently-developed \good{regularization} method
              that proved to be very effective''
    \end{itemize}
    \pause
    \vspace{1ex}
    \begin{center}
        \large
        \good{Not so different from today\ldots}
    \end{center}

    \source{https://image-net.org/challenges/LSVRC/2012/results.html\#abstract}
\end{frame}

\begin{frame}{Motivation: ImageNet Today}

    \begin{center}
        \large AlexNet won with \( \bad{84.69\%} \) top-five accuracy \citep{krizhevsky2012alexnet}.
    \end{center}

    \pause
    \horizontalrule

    \begin{center}
        \large Today, models get \( \good{99.02\%} \) top-5 accuracy \citep{yuan2021florence}!

        \vspace{3em}

        (Using all sorts of tricks like pre-training, transformers, etc.)
    \end{center}
\end{frame}

\begin{frame}{Motivation: DALL$\cdot$E 2}
    \begin{center}
        We've developed amazing deep learning tools since AlexNet.
    \end{center}
    \pause
    \vspace{1ex}

    \begin{figure}[]
        \centering
        \includegraphics[width=0.6\linewidth]{assets/bowl_of_soup.jpg}
        \caption*{Generated by DALL$\cdot$E 2}%
    \end{figure}

    \begin{center}
        \textit{A bowl of soup that is a portal to another
            dimension as digital art.}
    \end{center}

    \source{https://openai.com/dall-e-2/}

\end{frame}

\begin{frame}{Motivation: Cost of Training DALL$\cdot$E 2}

    \begin{center}
        \large
        DALL$\cdot$E 2 has 5.5 billion parameters and took \red{billions} of Adam
        iterations to fit \citep{ramesh2022dalle}.
    \end{center}

    \pause
    \horizontalrule

    \begin{center}
        \large
        But this is small compared to recent LLMs!
    \end{center}

    \pause
    \vspace{2ex}

    Consider OpenAI's \good{GPT-4 model}:
    \vspace{1ex}
    \pause

    \begin{itemize}
        \item GPT-4 has \bad{\( 1.8 \) trillion} parameters;
              \vspace{0.5ex}
              \pause

        \item It was trained on \bad{\( \approx 13 \) trillion} tokens;
              \vspace{0.5ex}
              \pause

        \item Training used \bad{25,000 A100s} for 90 to 100 days;
              \vspace{0.5ex}
              \pause

        \item At \$1 per A100 hour, GPT-4 cost \bad{\( \approx \$63 \) million} dollars.
    \end{itemize}

    \source{https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/}

\end{frame}

\begin{frame}{Motivation: Challenges of Non-Convexity}

    \textbf{Takeaway}:
    Modern deep learning models are \bad{huge} and \bad{extremely expensive}
    to train, but they have \good{tremendous impact}.
    \pause
    \vspace{1.0ex}

    \hspace{1em} \( \hookrightarrow \) ChatGPT is
    the fastest-adopted piece of software in history!

    \pause
    \horizontalrule
    \vspace{-1ex}

    \begin{figure}[]
        \centering
        \ifdefined\showtikz
            \input{assets/non_convex.tex}
        \else
            \Huge Non-Convex Figure
        \fi
    \end{figure}

    \vspace{-1ex}
    \textbf{Challenges of Non-Convexity}:
    \vspace{0.25ex}
    \pause

    \begin{itemize}
        \item \bad{Optimization}: saddle-points, local minima,
              slow convergence.

              \vspace{0.5em}
              \pause

        \item \bad{Optimality Conditions}: stationarity
              \( \centernot \implies \) optimality.

              \vspace{0.5em}
              \pause

        \item \bad{Mathematical Tools}: No subgradients, no separating
              hyperplanes, (usually) non-zero duality gap.
    \end{itemize}

    \source{www.theguardian.com/technology/2023/feb/02/chatgpt-100-million-users-open-ai-fastest-growing-app}

\end{frame}

\begin{frame}{Motivation: Convexity and Deep Learning}

    \textbf{Key Question}: How can we overcome \bad{non-convexity} to get
    \underline{better optimization} and \underline{fundamental theory} for neural networks?

    \pause
    \vspace{2ex}

    \textbf{This Thesis}: By creating and studying \good{convex reformulations}.

    \pause
    \vspace{2ex}

    \input{assets/relations.tex}

    \pause
    \vspace{1.5ex}

    \hspace{1em} \( \hookrightarrow \) This approach yields
    new \good{algorithms} and new \good{insights}!
\end{frame}

%\begin{frame}{Overview: Challenges of Non-Convexity}
%    \begin{center}
%        \Large
%        Non-convexity makes extensions to MLPs \bad{hard}!
%    \end{center}

%    %\begin{itemize}
%    %    \item \textbf{Certificates}: stationary points are global minima.

%    %          \pause
%    %    \item \textbf{Model Churn}: strict/strong convexity gives uniqueness.

%    %          \pause
%    %    \item \textbf{Tuning}: line-search, full-batch methods, acceleration, etc.
%    %\end{itemize}

%\end{frame}

\begin{frame}{Motivation: Two Layer Models}

    \begin{center}
        \large
        We primarily study \bad{two-layer} ReLU networks.
    \end{center}

    \pause
    \horizontalrule
    %\vspace{1ex}

    Two-layer ReLU models are \textbf{foundational building blocks}:
    \pause
    \vspace{1ex}
    \begin{itemize}
        \item They are the basic units of models like \good{MLPs} and \good{CNNs}\ldots
              \pause
              \vspace{0.5ex}

        \item \ldots and \good{layer-wise training} is fast and generalizes well
              \citep{belilovsky2019layer}.

              \pause
              \vspace{0.5ex}

        \item Two-layer networks can model \good{self-attention}
              \citep{sahiner2022attention}\ldots

              \pause
              \vspace{0.5ex}

        \item \ldots and are basic components of standard \good{transformer
                  blocks}.

              \pause
              \vspace{0.5ex}

        \item They were key to word-embeddings (\good{Word2Vec})
              \citep{mikolov2023word}\ldots
              \pause
              \vspace{0.5ex}

        \item \ldots and are widely used in \good{reinforcement learning}
              \citep{gaur2023global} and for prediction on \good{edge devices}
              \citep{teerapittayanon2017distributed}.

    \end{itemize}
    \pause
    \vspace{1ex}

    \begin{center}
        \large
        \textbf{More Importantly}: What can we achieve with two-layers?
    \end{center}

\end{frame}

\begin{frame}{Motivation: Tuning-Free Training Algorithms}

    Training neural networks involves many \bad{hyper-parameters}.
    \pause
    \vspace{0.5ex}
    \begin{itemize}
        \item \textbf{Step-Size}: too small \( \implies \) very slow convergence.
              \pause
              \vspace{0.5ex}
        \item \textbf{Step-Size}: too big \( \implies \) catastrophic failure.
              \pause
              \vspace{0.5ex}
        \item Not to mention batch-size, momentum, decay schedules, \ldots
    \end{itemize}
    \pause

    \begin{figure}
        \centering
        \includegraphics[width=0.8\textwidth]{assets/synthetic_classification.png}
    \end{figure}
    \pause

    Convex reformulations enable \good{parameter-free} global optimization!

\end{frame}

\begin{frame}{Motivation: Theory of Neural Networks}

    {\large \underline{Going Beyond Optimization}}
    \vspace{1.5ex}

    \begin{itemize}
        \item Better optimization is important for \textbf{practitioners}.
              \pause
              \vspace{1ex}

              \begin{itemize}
                  \item Less \bad{babysitting} means \good{faster, cheaper, better} training.
              \end{itemize}
              \pause
              \vspace{1.5ex}

        \item But expensive hyper-parameter tuning hasn't prevented neural
              networks from being deployed \textbf{everywhere}.
              \pause
              \vspace{1ex}

              \begin{itemize}
                  \item They're in our \good{cars} (self-driving), \good{phones} (Face ID),
                        \bad{classrooms} (ChatGPT), and even our \good{research} (ChatGPT again)!
              \end{itemize}
              \pause
              \vspace{1.5ex}

        \item Understanding neural networks is important for \textbf{everyone}.
              \pause
              \vspace{1ex}

              \begin{itemize}
                  \item Neural network theory is necessary for
                        \good{safety}, \good{reliability}, and \good{future
                            advances}.
              \end{itemize}
    \end{itemize}

    \hspace{1ex}
    \pause

    {\large Let's look at some examples\ldots}

\end{frame}

\begin{frame}{Motivation: Global Optima and Generalization}

    \begin{itemize}
        \item Suppose we could take 10,000 models from the set of
              \good{globally optimal} neural networks given a fixed training set.
              \pause
        \item \textbf{Q}: Are they going to generalize differently?
    \end{itemize}
    \pause

    \begin{figure}[t]
        \centering
        \includegraphics[width=0.48\linewidth]{assets/dist_paper_monks-1.pdf}
        \includegraphics[width=0.48\linewidth]{assets/dist_paper_planning.pdf}
    \end{figure}

    \pause

    \textbf{Conclusion}: We need to distinguish between global optima!

\end{frame}

\begin{frame}{Motivation: Structure of Solution Sets}

    \textbf{Q}: What is the structure of the optimal set?
    \pause
    \begin{itemize}
        \item Are the solutions \bad{discrete} or \good{highly connected}?
              \pause
        \item Are connected components \bad{disorganized} or very \good{structured}?
              \pause
    \end{itemize}
    
    \begin{columns}
        \column{0.4\textwidth} 
        \includegraphics[width=1\linewidth]{assets/convex_polytope.png}

        \textbf{A}: It's a convex polytope!

        \pause

        \column{0.65\textwidth}%
        \includegraphics[width=1.1\linewidth]{assets/solution_sets_vis_270.png}
        
        \vspace{1.5ex}
        \textbf{Non-Convex} solution set maps the polytope to a manifold.

    \end{columns}
    
    \vspace{2ex}
    \pause

    \hspace{1em} \( \hookrightarrow \) A \good{connectivity hierarchy}
    emerges with network width!
\end{frame}

%\begin{frame}{Motivation: Dispelling Myths}

%    \begin{center}
%        \large What can we do beyond better optimization algorithms?
%    \end{center}

%    \vspace{-2.5ex}
%    \pause
%    \horizontalrule
%    \vspace{-1ex}

%    We disprove \textbf{long-standing myths} in neural network theory!

%    \pause
%    \vspace{1ex}

%    \begin{itemize}
%        \item ``Optimal neural networks are unique up to model symmetries,
%              like permutations or neuron splitting''.
%              \pause
%              \vspace{0.5ex}
%              \begin{itemize}
%                  \item \bad{False!} But the optimal \good{neuron directions} are unique!
%              \end{itemize}
%              \pause
%              \vspace{0.75ex}

%        \item ``All optimal networks generalize the same''.
%              \pause
%              \vspace{0.5ex}
%              \begin{itemize}
%                  \item \bad{False!} The optimal set admits a \good{clear
%                            characterization} and distinct members of this set can
%                        generalize \good{very differently}.
%              \end{itemize}
%              \pause
%              \vspace{0.75ex}

%        \item ``All the globally optimal neural networks are connected''.
%              \pause
%              \vspace{0.5ex}
%              \begin{itemize}
%                  \item \bad{False!} But a \good{connectivity hierarchy}
%                        emerges as the network width increases.
%              \end{itemize}
%              \pause
%    \end{itemize}

%    \vspace{0.75ex}

%    We prove this and more by exactly characterizing the \textbf{optimal set}.

%\end{frame}

%\begin{frame}{Motivation: Compression and Neuron Pruning}

%    \textbf{Question}: What does the optimal set tell us about \bad{model
%        compression} and \bad{neuron pruning}?

%    \pause
%    \vspace{1ex}

%    \textbf{Answer}: Tight bounds on \good{model width} and an \good{optimal
%        neuron pruning} algorithm in poly-time.

%    \pause

%    \begin{figure}
%        \centering
%        \includegraphics[width=0.9\textwidth]{assets/mnist_pruning_acc.pdf}
%    \end{figure}

%\end{frame}

%\begin{frame}{Overview: Power of Convexity}

%    \begin{figure}
%        \centering
%        \includegraphics[width=0.9\textwidth]{assets/lars.pdf}
%    \end{figure}
%    \vspace{-0.5ex}

%    \textbf{Consider the Lasso:}
%    \pause
%    \vspace{0.5ex}
%    \begin{enumerate}
%        \item \textbf{Optimal Sets}: we have an exact \good{polyhedral
%                  characterization} and simple criteria for \good{uniqueness}
%              (general position) \citep{tibshirani2013unique}.
%              \pause
%              \vspace{0.5ex}

%        \item \textbf{Regularization Paths}: we know the (min-norm) solution
%              path is \good{continuous} and \good{piece-wise linear}
%              \citep{osborne2000new}.
%              \pause
%              \vspace{0.5ex}

%        \item \textbf{Algorithms}: we have efficient algorithms for
%              \good{homotopy} \citep{efron2004least} and computing \good{minimal
%                  solutions} \citep{tibshirani2013unique}.
%    \end{enumerate}

%\end{frame}{}

%\begin{frame}{Overview: Challenges of Non-Convexity}
%    \begin{center}
%        \Large
%        Non-convexity makes extensions to MLPs \bad{hard}!
%    \end{center}

%    \begin{figure}[]
%        \centering
%        \ifdefined\showtikz
%            \input{assets/non_convex.tex}
%        \else
%            \Huge Non-Convex Figure
%        \fi
%    \end{figure}

%    \pause
%    \begin{itemize}

%        \item \textbf{Optimization}: gradient methods escape saddle-points
%              \bad{slowly} and may converge to \bad{sub-optimal} local minima.
%              \vspace{1em}
%              \pause

%        \item \textbf{Optimality Conditions}: Stationarity
%              \( \centernot \implies \) optimality.
%              We have no global \bad{optimality criteria} and no \bad{certificates}.

%              \vspace{1em}
%              \pause

%        \item \textbf{Mathematical Tools}: We lose most of convex analysis
%              and have to work with \bad{Clarke stationary points}, etc.
%    \end{itemize}

%    %\begin{itemize}
%    %    \item \textbf{Certificates}: stationary points are global minima.

%    %          \pause
%    %    \item \textbf{Model Churn}: strict/strong convexity gives uniqueness.

%    %          \pause
%    %    \item \textbf{Tuning}: line-search, full-batch methods, acceleration, etc.
%    %\end{itemize}

%\end{frame}

\begin{frame}{Overview: Big Idea}

    {
        \large
        \bad{Overall Problem}: neural networks are hard to train and even
        harder to analyze because of non-convexity.
    }

    \pause
    \vspace{0.5em}
    \horizontalrule
    \vspace{0.5em}

    {
        \large \good{Thesis Goal}: leverage \textbf{convex reformulations}
        of neural networks to break the barrier of non-convexity and obtain,
        \vspace{1ex}
        \pause

        \begin{itemize}
            \item faster, more reliable optimization algorithms for training
                  shallow neural networks;
                  \pause
                  \vspace{1ex}

            \item a variational theory for the optimal set, solution path,
                  and stability of shallow neural network optimization;
                  \pause
                  \vspace{1ex}

            \item extensions to deep, fully-connected ReLU networks.
        \end{itemize}
    }
\end{frame}

\setbeamercolor{background canvas}{bg=LightCyan}
\begin{frame}{}
    \begin{center}
        \huge
        2. Background on Convex Reformulations
    \end{center}
\end{frame}
\setbeamercolor{background canvas}{bg=white}

\begin{frame}{Convex Reformulations: Flavor of Results}

    \textbf{Basic Idea}: We start with a \bad{non-convex} optimization problem
    and derive an equivalent \good{convex} program.

    \pause
    \vspace{2em}

    \textbf{Equivalent} means:
    \pause
    \vspace{0.5em}
    \begin{itemize}
        \item The global minima have the same values: \( p^* = q^* \)
              \vspace{1em}
              \pause
        \item We can map every global minimum \( u^* \) for one problem into
              a global minimum \( v^* \) of the other.
    \end{itemize}

\end{frame}

\begin{frame}{Convex Reformulations: Two-Layer ReLU Networks}

    {\large \bad{Non-Convex Problem} (NC-ReLU)}
    \[
        \min_{W_1, w_2} \underbrace{\half \norm{\sum_{j=1}^m (X W_{1j})_+ w_{2j} - y}_2^2}_{\text{Squared Error}}
        + \underbrace{\lambda \sum_{j=1}^m \norm{W_{1j}}_2^2 + |w_{2j}|^2}_{\text{Weight Decay}},
    \]
    where \( \rbr{z}_+ = \max\cbr{z, 0} \), \( X \in \R^{n \times d} \), and
    \( y \in \R^{n} \).
    \pause

    \begin{figure}[]
        \centering
        \ifdefined\showtikz
            \input{assets/neural_net}
        \else
            \Huge Neural Network Figure
        \fi
    \end{figure}

\end{frame}

\begin{frame}{Aside: ReLU Activation Patterns}

    Each ReLU neuron is active on a half-space: \( \rbr{X W_{1j}}_+ \)

    \pause

    \begin{figure}[]
        \centering
        \ifdefined\showtikz
            \input{assets/activation_pattern_0}
        \else
            \Huge activation pattern figure
        \fi
    \end{figure}

    \phantom{
        \textbf{Activation Pattern} satisfies \( D_j X W_{1j} = \rbr{X W_{1j}}_+ \)
    }

\end{frame}

\begin{frame}{Aside: ReLU Activation Patterns}

    Each ReLU neuron is active on a half-space: \( \rbr{X W_{1j}}_+ \)

    \begin{figure}[]
        \centering
        \ifdefined\showtikz
            \input{assets/activation_pattern_1}
        \else
            \Huge activation pattern figure
        \fi
    \end{figure}

    \pause
    \textbf{Activation patterns} linearize the ReLU:
    \( \textcolor{brightpink}{\rbr{X W_{1j}}_+ = D_j X W_{1j}} \).

\end{frame}

\begin{frame}{Aside: ReLU Activation Patterns}

    Each ReLU neuron is active on a half-space: \( \rbr{X W_{1j}}_+ \)

    \begin{figure}[]
        \centering
        \ifdefined\showtikz
            \input{assets/activation_pattern_2}
        \else
            \Huge activation pattern figure
        \fi
    \end{figure}

    \textbf{Activation patterns} linearize the ReLU:
    \( \textcolor{brightpink}{\rbr{X W_{1j}}_+ = D_j X W_{1j}} \).

\end{frame}

\begin{frame}{Convex Reformulations: Convex Problem}

    {\large \good{Convex Reformulation} (C-ReLU)} \citep{pilanci2020convex}
    \[
        \begin{aligned}
            \min_{v, w} & \half \norm{\sum_{j=1}^p \textcolor{brightpink}{D_j X (v_j - w_j)} - y}_2^2 +
            \lambda \sum_{j=1}^p \norm{v_j}_2 + \norm{w_j}_2                                            \\
                        & \hspace{0.2em} \text{s.t. }
            \textcolor{brightpink}{v_j, w_j \in \calK_j} :=
            \big\{w : \underbrace{(2D_j - I) X w \geq 0}_{\iff \rbr{X w}_+ = D_j X w}\big\},
        \end{aligned}
    \]
    where \( p \) is the number of unique activation patterns.
    \pause

    \begin{figure}[]
        \centering
        \ifdefined\showtikz
            \input{assets/convex_reformulation}
        \else
            \Huge convex reformulation figure
        \fi
    \end{figure}
\end{frame}

\begin{frame}{Convex Reformulations: Hardness}

    \textbf{Key Result}: if network width \( m \) satisfies \( m \geq m^* \)
    for some \( m^* \leq n \), then C-ReLU and NC-ReLU are \good{equivalent}
    \citep{pilanci2020convex}.

    \pause
    \horizontalrule

    How \textbf{hard} is the convex program?
    \pause

    \[
        p = \abs{\cbr{D_j = \text{diag}[\mathbbm{1}(X g_j \geq 0)] : g_j \in \R^d }}
    \]

    \vspace{2em}
    \pause

    The \textbf{convex program} is:
    \vspace{0.5em}
    \begin{itemize}
        \item \bad{Exponential in general}: \( p \in O(r \cdot (\frac{n}{r})^r) \),
              where \( r = \text{rank}(X) \).

              \pause
              \vspace{1em}

        \item \good{Highly structured} --- it's a linear model!
    \end{itemize}

    \vspace{1em}
    \pause

    \textbf{Takeaway}: We exchange one kind of hardness for another.

\end{frame}

\setbeamercolor{background canvas}{bg=LightCyan}
\begin{frame}{}
    \begin{center}
        \huge
        3. Better Optimization via Convex Reformulations
    \end{center}
\end{frame}
\setbeamercolor{background canvas}{bg=white}

\begin{frame}{Better Optimization via Convex Reformulations}

    \begin{center}
        \citep{mishkin2022fast} Fast Convex Optimization for Two-Layer ReLU Networks.
        \good{A.  Mishkin}, A.  Sahiner, M. Pilanci. ICML 2022.
    \end{center}

    \horizontalrule
    \pause

    \textbf{Big Idea}: Use convex reformulations as an \underline{optimization tool}.
    \vspace{1ex}
    \pause

    \begin{itemize}
        \item We could solve C-ReLU using \good{interior point methods}, but
              computing the Hessian is \bad{infeasible} for large \( n \) and \( d \).
              \pause
              \vspace{1ex}

        \item We could use \good{projected GD}, but projecting onto
              \( \calK_i \) is an \bad{expensive quadratic program}.
              \pause
              \vspace{1ex}

        \item Instead, we develop fast solvers based on the \good{augmented
                  Lagrangian method} and on a ``\good{gated ReLU}'' relaxation.
    \end{itemize}

    %\begin{itemize}
    %    \item \good{Gated ReLU Networks}: an unconstrained convex reformulation
    %          which is a super-class of ReLU neural networks.
    %          \pause
    %          \vspace{1ex}

    %    \item \good{Fast Optimizers}: reliable optimization (or approximation)
    %          of ReLU networks via gated ReLU networks.
    %          \pause
    %          \vspace{1ex}

    %    \item \good{Open-Source Code}: a public software package called
    %          \texttt{scnn} for training neural networks using our
    %          convex optimizers.
    %\end{itemize}

\end{frame}

\begin{frame}{Fast Training: Gated ReLU Networks}

    Recall the convex reformulation for a two-layer ReLU Network:

    \[
        \begin{aligned}
            \textbf{C-ReLU}: \min_{u} & \norm{\sum_{j=1}^p D_j X (v_j - w_j) - y}_2^2 +
            \lambda \sum_{j=1}^p \norm{v_j}_2 + \norm{w_j}_2                            \\
                                      & \hspace{0.2em} \bad{\text{s.t. }
                v_j, w_j \in \calK_j := \cbr{w : (2D_j - I) X w \geq 0},}
        \end{aligned}
    \]

    \pause
    \horizontalrule

    \textbf{Relaxation}: drop the cone constraints and simplify to obtain,
    \[
        \begin{aligned}
            \textbf{C-GReLU}: \min_{u} & \norm{\sum_{j=1}^p D_j X u_j - y}_2^2 +
            \lambda \sum_{j=1}^p \norm{u_j}_2                                    \\
        \end{aligned}
    \]

    \pause
    \textbf{Questions}:
    \pause
    \begin{enumerate}
        \item Is this still a neural network?
              \pause
              \vspace{1ex}

        \item When is it a good approximation for C-ReLU?
    \end{enumerate}

\end{frame}

\begin{frame}{Fast Training: Gated ReLU Networks}

    1. Is CG-ReLU equivalent to a neural network architecture?

    \pause
    \vspace{1ex}

    \begin{beamercolorbox}[wd=\textwidth,sep=1em]{result}
        \textbf{Theorem 2.2} \citep{mishkin2022fast}: C-GReLU is equivalent to
        a neural network with a ``Gated ReLU'' \citep{fiat2019decoupling}
        activation function.
        %\[ \phi_{g}(X, u) = \text{diag}(\mathbbm{1}(Xg \geq 0)) X u. \]
    \end{beamercolorbox}

    \pause
    \vspace{1.5ex}

    2. When does CG-ReLU give a good approximation for C-ReLU?
    \vspace{1ex}

    \pause

    \begin{beamercolorbox}[wd=\textwidth,sep=1em]{result}
        \textbf{Theorem 3.7} \citep{mishkin2022fast}:
        Let \( \lambda \geq 0 \) and let \( p^* \) be the optimal value of the ReLU problem.
        There exists a C-GReLU problem with minimizer \( u^* \) and optimal value \( d^* \) satisfying,
        \[
            d^* \leq p^* \leq d^* + \textcolor{Red}{2 \lambda \kappa(\tilde X_{\calJ}) \sum_{D_i \in \tilde \calD} \norm{u_i^*}_2}.
        \]
        \vspace{-2.5ex}
    \end{beamercolorbox}

\end{frame}

%\begin{frame}{Fast Training: Big Picture}
%    \textbf{Takeaways}:
%    \pause

%    \vspace{1ex}
%    \begin{itemize}
%        \item Gated ReLU is a super class of ReLU models.
%              \pause
%              \vspace{1ex}
%        \item We can convert between them at will.
%              \pause
%              \vspace{1ex}
%    \end{itemize}

%    \begin{figure}[]
%        \centering
%        \ifdefined\showtikz
%            \input{assets/relations}
%        \else
%            \Huge Relations Figure
%        \fi
%    \end{figure}

%\end{frame}

\begin{frame}{Fast Training: Solving the Convex Programs}
    We develop two algorithms for solving the convex reformulations:

    \vspace{1em}
    \pause

    \begin{itemize}
        \item \textbf{R-FISTA}: a restarted FISTA variant for Gated ReLU.
              \vspace{0.5em}
              \pause
        \item \textbf{AL}: an augmented Lagrangian method for the (constrained) ReLU Problem.
    \end{itemize}

    \pause
    \horizontalrule

    And we can use all the convex tricks!
    \vspace{1em}
    \pause
    \begin{itemize}
        \item \textbf{Fast}: \( O(1/T^2) \) convergence rate using \good{acceleration}.
              \vspace{0.5em}
              \pause

        \item \textbf{Tuning-free}: \good{line-search}, restarts, data normalization, \ldots
              \vspace{0.5em}
              \pause

        \item \textbf{Certificates}: \good{termination} based on min-norm subgradient.
    \end{itemize}

\end{frame}

\begin{frame}{Fast Training: Optimization Performance}

    We generate a performance profile using 438 training problems from the UCI repo.
    \pause

    \begin{figure}[t]
        \centering
        \includegraphics[width=1\linewidth]{assets/pp_main.pdf}
    \end{figure}

    \pause

    \begin{itemize}
        \item R-FISTA/AL solve more, faster, than SGD and Adam.
    \end{itemize}
\end{frame}

\setbeamercolor{background canvas}{bg=LightCyan}
\begin{frame}{}
    \begin{center}
        \huge
        4. Convex Reformulations for Theory of Neural Networks
    \end{center}
\end{frame}
\setbeamercolor{background canvas}{bg=white}

\begin{frame}{Optimal Sets: Summary}
    \begin{center}
        \citep{mishkin2023optimal} Optimal Sets and Solution Paths of {ReLU} Networks.
        \good{A. Mishkin}, M. Pilanci. ICML 2023
    \end{center}

    \horizontalrule
    \pause

    \textbf{Big Idea}: use convex reformulations as an \underline{analytical tool}
    to understand the set of all minimizers for two-layer ReLU networks,
    \vspace{2ex}

    \textbf{Non-Convex Solution Set (NC-ReLU)}:
    \[
        \begin{aligned}
            \calO^*(\lambda)
             & := \argmin_{W_1, w_2} \, \, \half \norm{\sum_{j=1}^m (X W_{1j})_+ w_{2j} - y}_2^2 \\
             & \hspace{6em} + \lambda \sum_{j=1}^m \norm{W_{1j}}_2^2 + |w_{2j}|^2,
        \end{aligned}
    \]

    %\begin{enumerate}
    %    \item \pause
    %          Characterize solutions to the \good{convex reformulation}
    %          using strong duality and KKT conditions.
    %          \vspace{1ex}
    %          \pause
    %    \item Extend results to \bad{non-convex} ReLU networks
    %          using the solution mapping.
    %          \vspace{1ex}
    %          \pause
    %    \item Leverage explicit characterization of the optimal
    %          set for \good{new insights and algorithms}.
    %          \vspace{1ex}
    %          \pause
    %\end{enumerate}

\end{frame}

%\begin{frame}{Optimal Set: Motivation}

%    \textbf{C-ReLU Solution Set}:
%    \[
%        \begin{aligned}
%            \solfn(\lambda) & =
%            \argmin_{v_i, w_i \in \calK_i} \, \bigg\{
%            \half \bigg\|\sum_{D_i \in \tilde \calD} D_i X (v_i - w_i), y\bigg\|_2^2                    \\
%                            & \quad \quad + \lambda\sum_{D_i \in \tilde \calD}\norm{v_i}_2+\norm{w_i}_2
%            \bigg\}.
%        \end{aligned}
%    \]

%    \pause
%    \vspace{0.5ex}
%    \horizontalrule

%    Why care about \( \solfn \)?

%    \pause
%    \vspace{1ex}

%    \begin{itemize}
%        \item \textbf{Generalization}: Different optimal networks can perform
%              differently at \good{test time}.
%              \pause
%              \vspace{1ex}

%        \item \textbf{Compression}: Some optimal models may be \good{smaller than
%                  others}.
%              \pause
%              \vspace{1ex}

%        \item \textbf{Stability}: necessary to understand \good{sensitivity to
%                  perturbations}, \( \solfn(\lambda + \epsilon) \).
%    \end{itemize}

%\end{frame}

\begin{frame}{Optimal Set: Strong Duality}

    \textbf{Convex Reformulation Solution Set (C-ReLU)}:
    \[
        \begin{aligned}
            \solfn(\lambda) & =
            \argmin_{v_i, w_i \in \calK_i} \, \bigg\{
            \half \bigg\|\sum_{D_i \in \tilde \calD} D_i X (v_i - w_i), y\bigg\|_2^2                    \\
                            & \quad \quad + \lambda\sum_{D_i \in \tilde \calD}\norm{v_i}_2+\norm{w_i}_2
            \bigg\}.
        \end{aligned}
    \]

    \pause
    \vspace{0.5ex}
    \horizontalrule

    \textbf{Approach:}

    \pause
    \vspace{1ex}

    \begin{enumerate}
        \item Convex objective + linear constraints \( \implies \) \good{strong duality}!
              \pause
              \vspace{1ex}

        \item We compute the optimal set using the \good{KKT conditions}.
              \pause
              \vspace{1ex}

        \item We then map back onto the \bad{non-convex parameterization}.
              \pause
              \vspace{1ex}

              \begin{itemize}
                  \item A little care is required to handle \bad{model symmetries}.
              \end{itemize}
    \end{enumerate}

    %This recipe applies to other variational properties including
    %\good{uniqueness}, continuity of the \good{regularization path},
    %\good{stability}, etc.

\end{frame}

\begin{frame}{Optimal Set: Characterization}
    \vspace{-2ex}
    \begin{itemize}
        \item
              \textbf{Optimal Fit:}
              \( \hat y := f_{\theta^*}(X) = \sum_{j=1}^m (X W_{1j}^*)_+ w_{2j}^*  \).
              \pause
              \vspace{0.5ex}

        \item
              \textbf{Block Correlations:} A collection of unique
              vectors \( q_i \), with one per activation pattern \( D_i \).
    \end{itemize}

    \vspace{-1ex}
    \pause
    \horizontalrule
    \vspace{-1ex}

    \begin{beamercolorbox}[wd=\textwidth,sep=1em]{result}
        \textbf{Theorem 4.1} \citep{mishkin2023optimal}
        Suppose \( m \geq m^* \).
        Then the optimal set for NC-ReLU up to
        \bad{permutation/split symmetries} is
        \vspace{-1ex}
        \begin{equation*}
            \begin{aligned}
                \hspace{-0.5em} \calO^*(\lambda)  = \,
                \big\{
                 & (W_1,  w_2) :
                \, \textcolor{brightpink}{f_{W_1, w_2}(X) = \hat y}, \\
                 & \forall \, \bi  \in  \calS_\lambda,
                \good{W_{1i} = (\sfrac{\alpha_{i}}{\lambda})^{\sfrac{1}{2}} q_i},
                \good{w_{2i} = (\alpha_i \lambda)^{\sfrac{1}{2}}},
                \alpha_i \geq 0                                      \\
                 & \forall \, \bi  \in [2p] \setminus \calS_\lambda,
                W_{1i} = 0, \, w_{2i} = 0
                \big\}.
            \end{aligned}
        \end{equation*}
        \vspace{-2ex}
    \end{beamercolorbox}
    \pause

    \begin{itemize}
        \item All optimal models have the same fit:
              \textcolor{brightpink}{\( f_{W_1, w_2}(X) = \hat{y} \)}.
              \pause
              \vspace{0.5ex}

        \item The neuron directions are unique:
              \good{\(
                  \frac{W_{1i}^*}{\norm{W_{1i}^*}_2} = q_i / \lambda_i.
                  \)}
    \end{itemize}

\end{frame}

\begin{frame}{Optimal Set: Appearance of Solutions}
    \begin{figure}[]
        \centering
        \includegraphics[width=0.96\textwidth]{assets/solution_sets_vis_270.png}
    \end{figure}

    The non-convex parameterization maps a \good{convex polytope} of
    solutions into a \bad{curved manifold}.

\end{frame}

\begin{frame}{Optimal Set: Exploration and Generalization}
    \begin{itemize}
        \item Take 10,000 samples from the set of optimal neural networks.
              \pause
        \item All samples have (i) \textbf{same training accuracy},
              (ii)~\textbf{same model norm}, but can \bad{generalize differently}.
              \pause
    \end{itemize}

    \begin{figure}[t]
        \centering
        \includegraphics[width=0.48\linewidth]{assets/dist_paper_monks-1.pdf}
        \includegraphics[width=0.48\linewidth]{assets/dist_paper_planning.pdf}
    \end{figure}

    \pause
    The solution you pick (\good{implicit regularization}) is crucial to
    good test performance!

\end{frame}

\begin{frame}{Optimal Set: Comparison to SGD/Adam}
    Fix \( \lambda = 10^{-5} \) and run SGD/Adam \( 1000 \) times
    with \textbf{independent initializations}.
    \pause

    \begin{figure}[t]
        \centering
        \includegraphics[width=0.48\linewidth]{assets/nc_comparison_monks-1.pdf}
        \includegraphics[width=0.48\linewidth]{assets/nc_comparison_planning.pdf}
    \end{figure}

    \pause
    \textbf{Note}: SGD/Adam can converge to \bad{local minima} which may
    perform better in this low-regularization setting.

\end{frame}

\begin{frame}{Neuron Pruning: Minimal Models}
    \textbf{Definition}: An optimal model is \good{minimal} if there
    does not exist another optimal model using a strict subset of active
    neurons.

    \vspace{1ex}
    \pause

    \begin{columns}
        \column{0.5\textwidth}
        \vspace{-1ex}
        \begin{figure}
            \centering
            \includegraphics[width=0.7\textwidth]{assets/polytope.png}
        \end{figure}

        \column{0.5\textwidth}
        \textbf{We prove}:
        \pause
        \begin{itemize}
            \item \good{Vertices} of the C-ReLU optimal set correspond exactly to minimal
                  models.
                  \pause
                  \vspace{1ex}

            \item There are at most \good{\( n \) neurons} in a minimal model.
                  \pause
                  \vspace{1ex}

            \item The \good{smallest minimal model} has exactly \( m^* \) neurons.
                  \begin{itemize}
                      \item \( m^* \iff \) minimum width for convex reformulation.
                  \end{itemize}
        \end{itemize}
    \end{columns}
    \pause
    \vspace{1.5ex}

    We also give a \good{poly-time algorithm} for computing minimal models.
    \vspace{0.5ex}
    \pause

    \hspace{1em} \( \hookrightarrow \) This is the first \underline{optimal pruning
        algorithm} for neural nets!

\end{frame}

\begin{frame}{Neuron Pruning: Performance on UCI Datasets}

    We also show how \textbf{optimal pruning} can be adapted to prune past \(
    m^* \) using a simple \good{correction step} (details in bonus!).

    \pause

    \begin{figure}[t]
        \centering
        \includegraphics[width=0.75\textwidth]{assets/uci_pruning_full_paper.pdf}
    \end{figure}
\end{frame}

\setbeamercolor{background canvas}{bg=LightCyan}
\begin{frame}{}
    \begin{center}
        \huge
        5. Extensions
    \end{center}
\end{frame}
\setbeamercolor{background canvas}{bg=white}

\begin{frame}{Extensions: Scalar Inputs}
    \begin{center}
        \citep{zeger2024library} A Library of Mirrors: Deep Neural Nets in
        Low Dimensions are Convex Lasso Models with Reflection Features. E.
        Zeger, Y. Wang, \good{A. Mishkin}, T. Ergen, E. Cand{\`{e}}s, M.
        Pilanci. SIMODS (\red{In Review}).
    \end{center}

    \horizontalrule
    \pause

    \textbf{Big Idea}: ReLU networks with one-dimensional inputs admit a
    simpler convex reformulation as \good{Lasso models}.
    \pause
    \vspace{1ex}

    \begin{itemize}
        \item The \good{feature matrix} for the Lasso model is
              determined by the model architecture.
              \pause
              \vspace{1ex}

        \item We extend our characterization of the set of \good{optimal ReLU
                  neural networks} to this setting.
    \end{itemize}
\end{frame}

\begin{frame}{Extensions: Mode Connectivity}
    \begin{center}
        \citep{kim2025exploring}
        Exploring The Loss Landscape Of Regularized Neural Networks Via Convex
        Duality. S. Kim, \good{A.  Mishkin}, M. Pilanci. ICLR 2025
        (\red{Oral}).
    \end{center}

    \horizontalrule
    \pause

    \textbf{Mode Connectivity}: how and when are optimal ReLU
    networks connected to each other in weight space?
    \pause
    \vspace{1ex}
    \begin{itemize}
        \item Our previous work assumed \( \bad{m \geq p} \): the
              width of the ReLU network is at least the number of activation
              patterns.
              \pause
              \vspace{1ex}
        \item Now we study the optimal set as \( m \) ranges from \( m^* \)
              to \( p \), creating a set of \good{transitions in connectivity}.
              %\pause
              %\vspace{1ex}

              %\item We also extend our optimal set results to \good{deep linear networks}
              %      and ReLU networks with \good{vector outputs}.
    \end{itemize}
\end{frame}

\begin{frame}{Mode Connectivity: Staircase of Connectivity}

    \begin{beamercolorbox}[wd=\textwidth,sep=1em]{result}
        \textbf{Theorem 2} \citep{kim2025exploring}:
        The critical widths \( m^* \) and \( M^* \) determine connectivity of
        the solution set:
        \pause
        \begin{enumerate}
            \item \( m = m^* \): the optimal set is a finite, \bad{fully
                      disconnected} set.
                  \pause

            \item \( m \geq m^* + 1 \): there exist at least \good{two solutions}
                  which are connected.
                  \pause

            \item  \( m = M^* \): there exists at least \bad{one disconnected
                      solution}.
                  \pause

            \item  \( m \geq M^* + 1\): \good{permutations} of each solution are connected.
                  There are no disconnected points.
                  \pause

            \item \( m \geq \min\cbr{n + 1, m^* + M^*} \): the optimal set is
                  \good{fully connected}.
                  \pause
        \end{enumerate}
    \end{beamercolorbox}
    \pause

    \begin{itemize}
        \item \textbf{Credit}: this theorem is due to Sungyoon Kim,
              building off of my optimal set work with Mert.
    \end{itemize}
\end{frame}

\begin{frame}{Mode Connectivity: Staircase in Action}
    \begin{center}
        \Large
        Staircase of Connectivity Visualized
    \end{center}
    \begin{figure}
        \centering
        \includegraphics[width=0.9\textwidth]{assets/staircase.png}
    \end{figure}
    \pause
    \vspace{2ex}

    \textbf{Takeaway}: Connectivity increases in phases with network width.
    \vspace{1ex}
    \pause

    \hspace{1em} \( \hookrightarrow \) This \good{definitively answers} a long
    standing question in the\\
    \hspace{2.5em} theory of neural networks!

\end{frame}

\begin{frame}{Extensions: Feature-Sparse Neural Networks}
    \begin{center}
        Convex LassoNet: Feature Sparse Convex Reformulations.
        \good{A.  Mishkin}, T. Ergen, F. Ruan, M. Pilanci, R. Tibshirani.
        (\red{Ongoing})
    \end{center}

    \horizontalrule
    \pause

    \textbf{Big Idea}: combine feature-sparsity with global optimization
    to \underline{improve generalization}.

    \pause
    \vspace{1ex}

    \begin{itemize}
        \item Naive approaches to feature sparsity in ReLU networks using group
              norms \bad{underperform} \citep{feng2017sparse}.
              \pause
              \vspace{0.5ex}

        \item But sophistical approaches like LassoNet
              \citep{lemhadri2021lassonet} are hard to train and can get
              trapped in \bad{local minima}.
              \pause
              \vspace{0.5ex}

        \item We derive \good{sparsity-inducing} convex reformulations with
              \good{global optimization} guarantees.
    \end{itemize}

\end{frame}

\begin{frame}{Feature-Sparsity: Planted Neural Networks}

    \begin{figure}
        \centering
        \includegraphics[width=0.75\textwidth]{assets/mip_relu_nn_100.pdf}
    \end{figure}

\end{frame}

\begin{frame}{Feature-Sparsity: Real Data}
    \begin{figure}
        \centering
        \includegraphics[width=0.75\textwidth]{assets/uci_regression_150.pdf}
    \end{figure}
\end{frame}

\begin{frame}{Extensions: Convexifying Deep Networks}
    \begin{center}
        Deep Convex Reformulations: Equivalences and Optimal Sets. \good{A.
            Mishkin}, M. Pilanci. (\red{Ongoing Work})
    \end{center}

    \horizontalrule
    \pause

    \textbf{Big Idea}: Extend convex reformulations to \good{deep ReLU
        networks} without relying on \bad{restricted architectures}.

    \pause
    \vspace{1ex}

    \begin{itemize}
        \item Three layer networks have \bad{non-linear}
              combinations of \bad{non-linear} functions, which are challenging
              to analyze.
              \pause
              \vspace{0.5ex}

        \item But, once we understand \good{three layer} networks, we understand
              \good{\( k \)-layer} networks for any \( k \geq 1 \).
              \pause
              \vspace{0.5ex}

        \item  We prove that ReLU MLPs of arbitrary depth are \good{convex
                  functions} with non-convex \bad{tensor decomposition} constraints.

    \end{itemize}

\end{frame}

\begin{frame}{Deep Networks: Layer Elimination}

    Let \( W^{(l)} \in \R^{d_{l-1} \times d_l} \) and
    consider the \( k \)-layer ReLU network
    \[
        f_\theta(X) = \rbr{\rbr{\rbr{X W^{(1)}}_+ W^{(2)}}_+ W^{(3)} \cdots }_+ W^{(k)}.
    \]

    \pause
    \horizontalrule

    \textbf{Question}: how do we construct a \good{convex reformulation}?

    \pause
    \vspace{1ex}

    \begin{enumerate}
        \item The first \good{two-layer block} has a convex reformulation in
              terms of the activation patterns \( D_i \),
              \[
                  f_\theta(X) = \rbr{\rbr{\sum_{i=1}^p D_i X T_i^{(2)} }_+ \otimes W_i^{(3)} \cdots }_+ W^{(k)}.
              \]
              \pause
              \vspace{0.5ex}

        \item This creates another \good{two-layer block}, which has a convex
              reformulation in terms of the activation patterns \( D_j^{(2)} \)\ldots
    \end{enumerate}

\end{frame}

\begin{frame}{Deep Networks: Tensor Programs}

    Let \( T^{(l)} \in \R^{d_0 \times \ldots d_{l}} \) be a tensor with
    low-rank decomposition,
    \[
        T^{(l)} \in \calG^{(l)} \approx \cbr{ T^{(l)} :
        \exists \, T^{(l-1)} \in \calG^{(l-1)} \text{ s.t. }
        T^{l}_i = T_i^{(l-1)} \otimes W^{(l)}_i.
        }
    \]
    \vspace{-3ex}
    \pause

    \begin{beamercolorbox}[wd=\textwidth,sep=1em]{result}
        \textbf{Proposition}:
        Training a \( k \)-layer ReLU model,
        \begin{equation*}
            \min_{\theta} \calL\rbr{f_{\theta}(X), y},
        \end{equation*}
        is equivalent to solving the order \( k+1 \) tensor program
        \begin{equation*}
            \min_{T^{(k)}} \, \calL\rbr{X^{(k)} \odot T^{(k)}, y}
            \quad \text{s.t.} \quad T^{(k)} \in \calG^{(k)}.
        \end{equation*}
        \vspace{-4ex}
    \end{beamercolorbox}
    \pause

    \begin{itemize}
        \item \( \text{rank}(T^{(k)}) \leq d_0 \prod_{l=1}^k b_l \),
              where \( b_l \leq d_l \) is the number of \good{unique activation
                  patterns} occurring in layer \( l \).
              \pause
              \vspace{0.5ex}

        \item If \( d_l \geq 2 p_l d_{l+1} \) for every \( l \), then this
              becomes \good{fully convex}.
    \end{itemize}

\end{frame}

\begin{frame}{Deep Networks: Tensor Parameterization}

    \textbf{Q}: What does the tensor \( T \) represent?
    \pause
    \vspace{1ex}

    \textbf{A}: Each entry \( T_{i_0, i_1, \ldots, i_{k}} \) is a
    path through the network DAG.
    \pause

    \input{assets/tensor_path.tex}

\end{frame}

\setbeamercolor{background canvas}{bg=LightCyan}

\begin{frame}{}
    \begin{center}
        \huge Big-Picture Summary
    \end{center}
\end{frame}
\setbeamercolor{background canvas}{bg=white}

\begin{frame}{Big-Picture Summary}

    \textbf{Summary}: We use convex reformulations for breakthroughs
    in training and neural network theory.
    \vspace{1ex}
    \pause
    \begin{itemize}
        \item \textbf{Fast Training}:
              We develop two fast, \good{tuning-free algorithms} to train (or
              approximate) two-layer ReLU networks.

              \pause
              \vspace{1ex}

        \item \textbf{Optimal Sets}:
              We characterize all \good{global optima} for training
              two-layer ReLU networks.
              \pause
              \vspace{1ex}

        \item \textbf{Deep Networks}:
              We show that that deep ReLU networks are \good{tensor programs}
              that convexify with increasing width.
              \pause
              \vspace{1ex}

        \item \textbf{Additional Results}:
              We provide many, many more results on \good{continuity},
              \good{stability}, \good{optimization}, and other areas.
    \end{itemize}

\end{frame}

\begin{frame}{Overview: Publications and Ongoing Projects}

    \textbf{Publications}:

    {\small
    \begin{enumerate}
        \item Fast Convex Optimization for Two-Layer ReLU Networks. \good{A.
                  Mishkin}, A. Sahiner, M. Pilanci. ICML 2022.

        \item Optimal Sets and Solution Paths of {ReLU} Networks. \good{A.
                  Mishkin}, M. Pilanci. ICML 2023.

        \item A Library of Mirrors: Deep Neural Nets in Low Dimensions are
              Convex Lasso Models with Reflection Features. E. Zeger, Y. Wang,
              \good{A. Mishkin}, T. Ergen, E. Cand{\`{e}}s, M. Pilanci. SIMODS
              (\red{In Review}).

        \item Exploring The Loss Landscape Of Regularized Neural Networks Via
              Convex Duality. S. Kim, \good{A.  Mishkin}, M. Pilanci. ICLR 2025
              (\red{Oral}).
    \end{enumerate}
    }

    \textbf{Ongoing Projects}:
    { \small
    \begin{enumerate}
        \item Deep Convex Reformulations: Equivalences and Optimal Sets

        \item Convex LassoNet: Feature-Sparse Convex Reformulations
    \end{enumerate}
    }

\end{frame}

\begin{frame}{Overview: Additional Projects (\red{Details in Bonus})}

    \textbf{Additional Publications}:
    {\small
    \begin{enumerate}
        \item Directional Smoothness and Gradient Methods: Convergence and
              Adaptivity. \good{A. Mishkin}*, A. Khaled*, Y. Wang, A. Defazio, R. M.
              Gower. NeurIPS 2024.

        \item  Level Set Teleportation: An Optimization Perspective. \good{A.
                  Mishkin}, A. Bietti, R. M. Gower. AISTATS 2025.

        \item Glocal Smoothness: Line Search can really help! C. Fox, \good{A.
                  Mishkin}, S. Vaswani, M. Schmidt. SIOPT (\red{In Review}).
    \end{enumerate}
    }

    \textbf{Further Ongoing Projects}:

    { \small
    \begin{enumerate}
        \item Greedy 2-Coordinate Updates for
              Equality-Constrained Optimization via Steepest Descent in the
              1-Norm. A. V. Ramesh*, \good{A. Mishkin}*, M. Schmidt, Y. Zhou, J.
              Lavington, J. She. (\red{To Be Submitted})

        \item Global Convergence of Gradient Flow on 1D Data with Sign Noise.
              \good{A. Mishkin}, F. Bach.
    \end{enumerate}
    }
\end{frame}

\begin{frame}{Acknowledgements}

    My PhD has benefited from collaborations with many \good{mentors}, friends,
    and fellow students. Many thanks to,
    \vspace{1ex}
    \pause

    \begin{itemize}
        \item \textbf{Stanford}: \underline{\good{Mert Pilanci}},
              Sungyoon Kim, Tolga Ergen (LG AI), Arda Sahiner (Arcus), Emi
              Zeger, and many others.
              \pause
              \vspace{1ex}

        \item \textbf{Voleon}: \good{Sahand Negahban}, \good{Deepak Rajan},
              Alex Appleton.
              \pause
              \vspace{1ex}

        \item \textbf{Inria Paris}: \good{Francis Bach}, Frederik Kunstner.
              \pause
              \vspace{1ex}

        \item \textbf{The Flatiron Institute}:  \good{Alberto Bietti},
              \good{Robert Gower}, Aaron Defazio (Meta FAIR), Ahmed Khaled
              (Princeton), Yuanhao Wang (Princeton).
              \pause
              \vspace{1ex}

        \item \textbf{UBC}: \good{Mark Schmidt}, Jonathan Lavington (Amazon),
              Si Yi Meng (Cornell), Sharan Vaswani (SFU).
              \pause
              \vspace{1ex}

        \item \textbf{RIKEN AIP}: \good{Emtiyaz Khan}, Didrik Nielsen (Twig).
    \end{itemize}

\end{frame}

\begin{frame}{Acknowledgements Continued}

    Lastly, I want to make special mention of the following people:

    \pause
    \vspace{1ex}

    \begin{itemize}
        \item \good{My committee members}: Mert Pilanci, Stepehn Boyd, Mykel
              Kochenderfer, Balaji Prabhakar, Aaron Sidford, and Robert
              Tibshirani.
              \pause
              \vspace{1ex}

        \item My partner, \good{Sophie Boerlage}, for believing in me
              throughout this long journey.
              \pause
              \vspace{1ex}

        \item \good{Frederik Kunstner}, without whom I probably wouldn't have
              stumbled into optimization.
              \pause
              \vspace{1ex}

        \item And, of course, my \good{family}, whose constant support made
              all of this possible.
    \end{itemize}

\end{frame}

%% main content ends %%

%% bibliography
\begin{frame}[allowframebreaks]{References}
    \printbibliography[]
\end{frame}

\input{sections/bonus_optimization}

\input{sections/bonus_sol_fn}

\end{document}
